\begin{abstract}
    As phishing detection systems evolve from traditional classifiers to transformer-based architectures, understanding model behaviour has become a critical security priority. This systematic literature review investigates the application of Explainable AI (XAI) frameworks—specifically LIME and SHAP—in addressing transparency challenges in phishing detection. Synthesizing evidence from 12 primary experimental studies and 2 comprehensive surveys (2016--2025), the review assesses explanation fidelity, attribution stability, and human-centred utility. Findings indicate that while post-hoc surrogate explanations demonstrate strong fidelity for ensemble-based models, they exhibit instability and inconsistent token-level attributions in transformers such as BERT and RoBERTa. An evaluation gap is also identified, as many studies emphasize qualitative visual coherence over quantitative metrics of fidelity and robustness. Although XAI methods provide a theoretical foundation for transparency, their practical utility in professional security environments remains limited. Future research should establish standardized fidelity benchmarks and integrate human-in-the-loop evaluations to reduce analyst cognitive load and enable trustworthy deployment.
\end{abstract}